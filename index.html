---
layout: default
---

<body>	
	
<div class="content heading anchor" id="home">
		<table style="margin-top: -4mm;">
		</table>
        <div style="margin-top: 0mm;" class="img">
			<img class="header-img" src="images/anyirao.jpg" alt="Photo" align="left" height="240" >
		</div>
        <div style="margin-top: 7mm;" class="header-text">
            <h2>Anyi Rao</h2>
            <p>
				Assistant Professor <br> 
				Human AI for Creativity <br>
				Hong Kong University of Science and Technology <br>
				Associate Director of HKUST Media Intelligence Research Center<br>
				
				Email: anyirao [at] ust.hk <br>
            </p>
		<div id="contact">	
				<a href="https://scholar.google.com/citations?user=8lKr7j4AAAAJ&hl=en&authuser=1&oi=ao" class="icon">
          			<img src="images/ico/google_scholar_30.jpg"  alt="View Anyi Rao's profile on Google Scholar">
          		</a>
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          		<a href="https://github.com/AnyiRao" class="icon">
          			<img src="images/ico/git_30.jpg"  alt="View Anyi Rao's codes on Github">
          		</a>
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="https://www.linkedin.com/in/anyirao/" class="icon">
                		<img src="images/ico/linkedin_30.jpg" alt="View Anyi Rao's profile on LinkedIn">
				</a>
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="https://x.com/raoanyi" class="icon">
          			<img src="images/ico/x.png"  alt="View Anyi Rao's codes on X">
          		</a>
        </div>
	  </div>
</div>	   

<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>

	   
<div class="content anchor" id="bio">     
	<h3 class="content-section">Bio</h3>

		<p style="text-align: justify">	Anyi Rao is an Assistant Professor 
			<!-- in the Division of Arts and Machine Creativity (AMC) and the Division of Emerging Interdisciplinary Areas (EMIA)  -->
			<!-- at the <a style="color:#333" href="https://ais.hkust.edu.hk/">Academy of Interdisciplinary Studies (AIS)</a> -->
			at the Hong Kong University of Science and Technology (HKUST).
			He leads the <a style="color:#333" href="https://mmlab-hkust.github.io/">Multimedia Creativity Lab (MMLab@HKUST)</a>.
			He is the Associate Director of 
			<a style="color:#333" href="https://hkust-mirc.github.io/">HKUST Media Intelligence Research Center</a>.


			He studies human AI and agentic AI,
			focusing on the creation, editing and understandingand of art, media and film,
			aiming to build human-AI collaborative intelligence 
			and unleash human creativity and productivity.
			His works include <strong>
				<a style="color:#333" href="https://github.com/lllyasviel/ControlNet">ControlNet</a>, 
				<a style="color:#333" href="https://github.com/guoyww/AnimateDiff">AnimateDiff</a>, 
				<a style="color:#333" href="https://movienet.github.io">MovieNet</a>, 
				<a style="color:#333" href="https://virtualfilmstudio.github.io">Virtual Studio</a>, 
				and <a style="color:#333" href="https://github.com/lllyasviel/IC-Light">IC-Light</a>,
				</strong> 
				with a Marr Prize (ICCV best paper award).
			These works have been widely used in industry, including Amazon Prime Video, Netflix, Tencent, and more.
		<br>

		<p style="text-align: justify"> 
			He was a Postdoctoral Scholar at 
			<a href="https://www.stanford.edu/">Stanford</a>
			with 
			<a href="http://graphics.stanford.edu/~maneesh">Maneesh Agrawala</a>.
			He received the Ph.D. at <a href="http://mmlab.ie.cuhk.edu.hk">MMLab</a>,
			<a style="color:#333" href="http://csrankings.org/#/index?vision&world">Chinese University of Hong Kong</a> 
			with <a href="http://dahua.me">Dahua Lin</a> and <a href="https://boleizhou.github.io">Bolei Zhou</a>. 
			He has research experiences at 
			<a style="color:#333" href="https://about.meta.com/realitylabs/">Meta Reality Lab</a>, 
			<a style="color:#333" href="https://vectorinstitute.ai">Vector Institute</a>, 
			<a style="color:#333" href="https://web.cs.toronto.edu">University of Toronto</a>, 
			<a style="color:#333" href="https://www.cs.hku.hk">Hong Kong University</a>.
			
			He organized the SIGGRAPH/CVPR/ICCV/ECCV Creative Visual Content Workshop and
			the SIGGRAPH Generative Models Course, 
			curated 2025 Hong Kong HKUST AI Film Festival and 2023 Paris ShortFest AI Film Festival. 
			He also serves as a co-chair of MMSys26, UIST25, VINCI25, CVM25, UIST24 and 
			area chair/TPC of CVPR26, ICLR26, SIGGRAPH Asia26, UIST26, SIGGRAPH Asia25.
			
		<p style="text-align: justify"> 
			He has hosted the Brown Media Innovation Research Fund, Amazon Video Research Fund, 
			been featured in
			<a style="color:#333" href="https://www.forbes.com/profile/rao-anyi/?list=30under30-asia-healthcare-science/">Forbes 30 Under 30 Asia 2025 List</a>,
			and won the Rising Star Award at the World Artificial Intelligence Conference 2024.
			He gave keynote at the Golden Rooster Film Festival, the Shanghai Television Magnolia Festival, 
			was featured by Shanghai TV Financial Channel, Hong Kong Cable Television.
		
			<p style="text-align: justify"> <i>
				<!-- <a style="color:#f15959">[2025-02 Updated] Due to more funding available, the lab still have a few openings for 2025 fall intake.</a> -->
				Actively looking for highly motivated students to join the group. 
				See <a href="./openings">openings</a> for more details. 
				Please fill out this 
				<!-- <a href="https://forms.gle/bxQMUzWE4J91Nnn88"> 2025 form</a> for 2025 intake and  -->
				<a href="https://forms.gle/RVXjHiJxC1SzKN6x6"> 2026 form</a> for 2026 intake
				and send an email to me if you are interested in.</i>
			
			<!-- If you also have some exciting ideas and insights on the aforementioned research, please fill in this  
				 or drop me an email. Let's push it forward together. -->
			
			
			<!-- He was fortunate to work with 
			<a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>, 
			<a style="color:#333" href="https://web.cs.toronto.edu/">UofT</a> & 
			<a style="color:#333" href="https://vectorinstitute.ai/">Vector Institute</a>,
			<a href="http://ix.cs.uoregon.edu/~dou/">Dejing Dou</a>,
			<a href="http://ix.cs.uoregon.edu/~lowd/">Daniel Lowd</a>
			at <a style="color:#333" href="http://aimlab.cs.uoregon.edu/publication.php">AIM Lab</a>, 
			and <a href="http://www.cs.hku.hk/people/profile.jsp?teacher=fcmlau">Francis Lau</a>
			at <a style="color:#333" href="https://www.cs.hku.hk/">HKU CS</a>. -->
		</p>
</div>   
	
<div class="content anchor" id="news">  
	<h3 class="content-section-list">News</h3>
	<p>
	<li>
		2025-07: CineVision is accepted to UIST 2025 and Light-A-Video is accepted to ICCV 2025.
	<li>
		2025-05:
		We are organizing the <a href="https://cveu.github.io">Seventh Workshop on AI for Creative Visual Content Generation Editing and Understanding</a> at SIGGRAPH 2025.
		This is the first year that SIGGRAPH has a technical workshop program.
		Please follow our <a href="https://twitter.com/cveu_workshop">Twitter</a> for more information!
	<li>
		2025-04: Chair and curate the <a href="https://cveu.github.io/event/hkustfilm2025.html">First Hong Kong HKUST AI Film Festival </a> with CVM 2025 on April 19.
	
	<li>
		2025-02: IC-Light is accepted to ICLR 2025 as Oral.
		<iframe src="https://img.shields.io/github/stars/lllyasviel/IC-Light?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		And one paper is accepted to CVPR 2025.

	<li>
		2024-08:
		We are organizing the 
		<a href="https://cveu.github.io/event/sig2024">Course on Generative Models for Visual Content Editing and Creation</a> 
		at SIGGRAPH 2024.
	<li>
		2024-08:
		Three paper are accepted to UIST 2024, ICML 2024 and ECCV 2024.
	<li>
		2024-02: Cinematic Behavior Transfer is accepted to CVPR 2024. 
			Our efforts on Intelligent Cinematography üé¨
			<a href="https://virtualfilmstudio.github.io/">Virtual Film Studio</a> include our
			SIGGRAPH Virtual Dynamic Storyboard, 
			CVPR Cinematic Behavior Transfer and ECCV Multi-camera Editing.
	<li>
		2023-11: AnimateDiff is <a href="https://github.com/guoyww/AnimateDiff">online</a>
		<iframe src="https://img.shields.io/github/stars/guoyww/AnimateDiff?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		and gets an update on SparseCtrl ability.
	<li>
		2023-10: üßë‚Äçüé® ControlNet receives the üèÜ Best Paper Award (Marr Prize)  at ICCV 2023. 
		<a href="https://github.com/lllyasviel/ControlNet">V1</a>
		<iframe src="https://img.shields.io/github/stars/lllyasviel/ControlNet?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		<a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly">V1.1</a>
		<iframe src="https://img.shields.io/github/stars/lllyasviel/ControlNet-v1-1-nightly?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		<a href="https://github.com/Mikubill/sd-webui-controlnet">A1111 WebUI</a>
		<iframe src="https://img.shields.io/github/stars/Mikubill/sd-webui-controlnet?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
	
	<li>
		2023-06:
		We are organizing üçø inaugural Paris <a href="https://filmfreeway.com/AIShortFest">ShortFest</a> AI Film Festival, 
		jointly with ICCV 2023 in Paris, France.
	
	<details><summary>More</summary>
	<li>
		2025-02:
		We are organizing the <a href="https://cveu.github.io/event/cvpr2025.htmlo">Sixth Workshop on AI for Creative Visual Content Generation Editing and Understanding</a> at CVPR 2025.
	<li>
		2024-10: Serve in <a href="https://uist.acm.org/2025">ACM UIST 2025</a> organizing committee as a Registeration Co-Chair.
	<li>
		2024-02: Serve in <a href="https://uist.acm.org/2024">ACM UIST 2024</a> organizing committee as a Registeration Co-Chair.
	<li>
		2024-01:
		We are organizing the <a href="https://cveu.github.io/event/cvpr2024">Fourth Workshop on AI for Creative Visual Content Generation Editing and Understanding</a> at CVPR 2024.
	<li>
		2023-08: Three papers are accepted to ICCV 2023 as Oral, UIST 2023, and ACM MM 2023.
	<li>
		2023-06:
		We are organizing 
		the <a href="https://cveu.github.io/event/iccv2023">Third Workshop on AI for Creative Video Editing and Understanding</a> at ICCV 2023 in <a href="https://iccv2023.thecvf.com/">Paris</a>
	<li>
		2023-04: Two papers are accepted to AAAI 2023 as Oral and IJCAI 2023.
	<li>
		2022-10:
		We are organizing the <a href="https://cveu.github.io/event/eccv2022"> Second Workshop on AI for Creative Video Editing and Understanding</a> at ECCV 2022.

	<li>
		2022-07: Two papers on üë∑ <a href="https://city-super.github.io/">City-Super Research</a>: 
		<a href="https://city-super.github.io/citynerf/">CityNeRF</a> and 
		<a href="https://city-super.github.io/shoot360/">Shoot360</a> are accepted to ECCV 2022 and SIGGRAPH 2022.
	<li>
		2022-03: Two papers are accepted to CVPR 2022 and IEEE Transactions on Multimedia.	
	<li>
		2021-09: We are organizing the <a href="https://cveu.github.io/event/iccv2021"> First Workshop on AI for Creative Video Editing and Understanding</a> during ICCV 2021. 
	<li>
		2021-07: Two papers are accepted to ICCV 2021 and IEEE Transactions on Multimedia.
	<li> 
		2021-05: Our CVPR 2020 work <a href="https://github.com/AnyiRao/SceneSeg">SceneSeg</a>
		is set as the baseline for the 
		<a href="https://algo.qq.com/">ACM Multimedia 2021 Grand Challenge</a>: Tencent Ads Algorithm Competition.
		Participate to win USD$100,000 for the first prize.
	<li> 
		2020-07: <a href="https://movienet.github.io/">MovieNet</a> 
		is online with an easy-to-use <a href="https://github.com/movienet/movienet-tools">toolkit</a> 
		as a part of <a href="http://openmmlab.com/"> OpenMMLab</a>.
	<li> 
		2020-07: Three papers are accepted to ECCV 2020.
	<li> 
		2020-02: One paper is accepted to CVPR 2020.
		Also appears at <a href="https://sites.google.com/view/luv2020/home"> LUV 2020</a> (15-min talk)
		and <a href="http://sightsound.org"> Sight and Sound 2020</a> (5-min talk).
	<li> 
		2020-01: <a href="https://www.aclweb.org/anthology/P18-2006"> HotFlip </a> is included in <a href="https://allennlp.org"> AllenNLP</a>
		<iframe src="https://img.shields.io/github/stars/allenai/allennlp?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		and 
		<a href="https://github.com/QData/TextAttack"> TextAttack</a>
		<iframe src="https://img.shields.io/github/stars/QData/TextAttack?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
	</li>
	</details>
	</p>
</div>   
	
<div class="content anchor" id="publication">     
	<h3  class="content-section">
	Selected Publication <a style="color:#333333" href="./publication">[Full List]</a>
	</h3>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/uist25cinevision.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">CineVision: An Interactive Pre-visualization Storyboard System for Director‚ÄìCinematographer Collaboration </span><br>
			Zheng Wei, Hongtao Wu, Lvmin Zhang, Xian Xu, Yefeng Zheng, Pan Hui, Maneesh Agrawala, Huamin Qu, <span style="font-weight: bold;">Anyi Rao</span>
			<br>
			User Interface Software and Technology 
			(<span style="font-weight: bold;">UIST</span>), 2025 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2507.20355">[Paper]</a></span>
			<span class="tag"><a href="">[Webpage]</a></span>
		</td>
	</tbody>
	</table>


	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/cvpr25repainter.jpg" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">VideoRepainter: Keyframe-Guided Creative Video Inpainting </span><br>
			Yuwei Guo, Ceyuan Yang, <span style="font-weight: bold;">Anyi Rao</span>, Chenlin Meng, Omer Bar-Tal, Shuangrui Ding, Maneesh Agrawala, Dahua Lin, Bo Dai
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition 
			(<span style="font-weight: bold;">CVPR</span>), 2025 <br>
			<span class="tag"><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Keyframe-Guided_Creative_Video_Inpainting_CVPR_2025_paper.pdf">[Paper]</a></span>
			<span class="tag"><a href="https://guoyww.github.io/projects/VideoRepainter/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/iclr25iclight.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">IC-Light: Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport </span><br>
			Lvmin Zhang, <span style="font-weight: bold;">Anyi Rao</span>, Maneesh Agrawala
			<br>
			International Conference on Learning Representations
			(<span style="font-weight: bold;">ICLR</span>), 2025 <font color="#BF40BF">(Oral)</font> <br>
			<span class="tag"><a href="https://openreview.net/forum?id=u1cQYxRI1H">[Paper]</a></span>
			<span class="tag"><a href="https://github.com/lllyasviel/IC-Light">[Webpage]</a></span>
			<span class="tag"><a href="https://huggingface.co/spaces/lllyasviel/iclight-v2">[Demo]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/uist24script.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> ScriptViz: A Visualization Tool to Aid Scriptwriting based on a Large Movie Database
			</span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Jean-Pe√Øc Chou, Maneesh Agrawala
			<br>
			User Interface Software and Technology 
			(<span style="font-weight: bold;">UIST</span>), 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2410.03224">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/projects/scriptviz/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/arxiv23cinetransfer.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> Cinematic Behavior Transfer via NeRF-based Differentiable Filming
			</span><br>
			Xuekun Jiang*, <span style="font-weight: bold;">Anyi Rao* (co-first)</span>, Jingbo Wang, Dahua Lin, Bo Dai
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition 
			(<span style="font-weight: bold;">CVPR</span>), 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2311.17754">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/projects/cinetransfer/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>
	
	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/arxiv23animatediff.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;"> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning </span><br>
			 Yuwei Guo, Ceyuan Yang, <span style="font-weight: bold;">Anyi Rao</span>, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, Bo Dai
			<br>
			International Conference on Learning Representations
			(<span style="font-weight: bold;">ICLR</span>), 2024 <font color="#BF40BF">(Spotlight)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2307.04725">[Paper]</a></span>
			<span class="tag"><a href="https://github.com/guoyww/animatediff/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/arxiv23sparsectrl.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models
			</span><br>
			Yuwei Guo, Ceyuan Yang, <span style="font-weight: bold;">Anyi Rao</span>, Maneesh Agrawala, Dahua Lin, Bo Dai
			<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCV</span>), 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2311.16933">[Paper]</a></span>
			<span class="tag"><a href="https://guoyww.github.io/projects/SparseCtrl/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/iccv23control.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models </span><br>
			Lvmin Zhang, <span style="font-weight: bold;">Anyi Rao</span>, Maneesh Agrawala
			<br>
			IEEE/CVF International Conference on Computer Vision
			(<span style="font-weight: bold;">ICCV</span>), 2023 <font color="#BF40BF">Best Paper Award (Marr Prize)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2302.05543">[Paper]</a></span>
			<span class="tag"><a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly">[Webpage]</a></span>
			<span class="tag"><a href="https://lllyasviel.github.io/misc/202309/cnet_supp.pdf">[Supplements]</a></span>
			<span class="tag"><a href="https://github.com/lllyasviel/ControlNet">[V1]</a></span>
			<span class="tag"><a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly">[V1.1]</a></span>
			<span class="tag"><a href="https://github.com/Mikubill/sd-webui-controlnet">[A1111 WebUI]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/uist23lyrics.png" width="160"  border="0"></p>
			</td>
		<td  valign="middle">
		<p><span style="font-weight: bold;"> Automated Conversion of Music Videos into Lyric Videos </span><br>
				Jiaju Ma, <span style="font-weight: bold;">Anyi Rao</span>, Li-Yi Wei, Rubaiat Habib Kazi, Hijung Valentina Shin, Maneesh Agrawala
				<br>
				User Interface Software and Technology 
				(<span style="font-weight: bold;">UIST</span>), 2023 <br>
				<span class="tag"><a href="https://arxiv.org/abs/2308.14922">[Paper]</a></span>
				<span class="tag"><a href="https://majiaju.io/lyric-video">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/sig23vds.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;"> Dynamic Storyboard Generation in an Engine-based Virtual Environments for Video Production </span><br>
			<span style="font-weight: bold;">Anyi Rao*</span>, Xuekun Jiang*, Yuwei Guo, Linning Xu, Lei Yang, Libiao Jin, Dahua Lin, Bo Dai
			<br>
			ACM Special Interest Group on Computer Graphics and Interactive Techniques Conference
			(<span style="font-weight: bold;">SIGGRAPH</span>) Poster, 2023 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2301.12688">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<!-- <table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/aaai23pstl.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;"> Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences</span><br>
			Yujie Zhou, Haodong Duan, <span style="font-weight: bold;">Anyi Rao</span>, Bing Su, Jiaqi Wang
			<br>
			AAAI Conference on Artificial Intelligence
			(<span style="font-weight: bold;">AAAI</span>), 2023 <font color="#BF40BF">(Oral)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2302.09018">[Paper]</a></span>
			<span class="tag"><a href="https://github.com/YujieOuO/PSTL.git">[Webpage]</a></span>
		</td>
	</tbody>
	</table> -->

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/siggraph22shoot360.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;"> Shoot360: Normal View Video Creation from City Panorama Footage	</span><br>
			 <span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Dahua Lin
			<br>
			ACM Special Interest Group on Computer Graphics and Interactive Techniques Conference
			(<span style="font-weight: bold;">SIGGRAPH</span>), 2022 <br>
			<span class="tag"><a href="https://dl.acm.org/doi/abs/10.1145/3528233.3530702">[Paper]</a></span>
			<span class="tag"><a href="https://city-super.github.io/shoot360/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	
	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv22multi.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;">Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows
			</span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Xuekun Jiang, Sichen Wang, Yuwei Guo, Zihao Liu, Bo Dai, Long Pang, Xiaoyu Wu, Dahua Lin, Libiao Jin
			<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCVW</span>), 2022 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2210.08737">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/projects/multicam/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
			<img src="./images/papers/tmm22unscreen.png" width="160"  border="0"> </p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold; margin-top:5mm">  A Coarse-to-Fine Framework for Automatic Video Unscreen </span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Zhizhong Li, Qingqiu Huang, Zhanghui Kuang, Wayne Zhang, Dahua Lin 
			<br>
			IEEE Transactions on Multimedia, 
			(<span style="font-weight: bold;">TMM</span>), 2022 <br>
			<span class="tag"><a href="https://ieeexplore.ieee.org/document/9709668">[Paper]</a></span>
			<span class="tag"><a href="https://anyirao.com/files/projects/tmm22unscreen/">[Webpage]</a></span>
			</p>
		</td>
	</tbody>
	</table>

	
	<table style="margin-top: -2mm;"  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:3mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv22bungeenerf.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;">BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering</span> 
			<br>
				Also known as <span style="font-weight: bold;">CityNeRF</span><span style="font-weight: itaic;">: Building NeRF at City Scale</span> 
			<br>
			Yuanbo Xiangli*, Linning Xu*, Xingang Pan, Nanxuan Zhao, <span style="font-weight: bold;">Anyi Rao</span>, Christian Theobalt, Bo Dai, Dahua Lin
			<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCV</span>), 2022 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2112.05504"> [Paper]</a></span>
			<span class="tag"><a href="https://city-super.github.io/citynerf">[Webpage]</a></span>
		</td>
		</tbody>
	</table>

	

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/cvpr20sceneseg.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">A Local-to-Global Approach to Multi-modal Movie Scene Segmentation </span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition 
			(<span style="font-weight: bold;">CVPR</span>), 2020 <br>
			<a href="https://sites.google.com/view/luv2020/home"> LUV 2020</a> 
			and <a href="http://sightsound.org/"> Sight and Sound 2020</a> workshops <font color="#BF40BF">(Oral)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2004.02678">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/projects/cvpr20sceneseg/">[Webpage]</a></span>
		</td>
		</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv20shot.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;">A Unified Framework for Shot Type Classification Based on Subject Centric Lens
			</span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, Dahua Lin
			<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCV</span>), 2020 <br>
			<a href="https://dramaqa.snu.ac.kr/Workshop/2020"> Video Turing Test 2020</a> Workshop <font color="#BF40BF">(Oral)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2008.03548">[Paper]</a></span>
			<span class="tag"><a href="https://virtualfilmstudio.github.io/projects/eccv20shot/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>



	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:3mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv20movienet.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;">MovieNet: A Holistic Dataset for Movie Understanding 
			</span><br>
			Qingqiu Huang, Yu Xiong, <span style="font-weight: bold;">Anyi Rao</span>, Jiaze Wang, Dahua Lin
			<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCV</span>), 2020 <font color="#BF40BF">(Spotlight)</font> <br>
			<span class="tag"><a href="https://arxiv.org/abs/2007.10937">[Paper]</a></span>
			<span class="tag"><a href="http://movienet.github.io/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<!-- <table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
    <tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/acl18hotflip.png" width="160"  border="0"></p>
        </td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">HotFlip: White-Box Adversarial Examples for Text Classification</span> <br>
			Javid Ebrahimi, <span style="font-weight: bold;">Anyi Rao</span>, Daniel Lowd, Dejing Dou <br>
			Annual Meeting of the Association for Computational Linguistics
			(<span style="font-weight: bold;">ACL</span>), 2018 <br>
			It is included in several open-source NLP research libraries <a href="https://github.com/allenai/allennlp"> AllenNLP</a>,
			<a href="https://github.com/QData/TextAttack"> TextAttack</a>
			and <a href="https://github.com/thunlp/OpenAttack"> OpenAttack</a>
			<br>
			<span class="tag"><a href="https://aclanthology.org/P18-2006.pdf"> [Paper]</a></span>
			<span class="tag"><a href="http://www.aclweb.org/anthology/P18-2006"> [Webpage]</a></span>
			<span class="tag"><a href="https://github.com/allenai/allennlp/blob/master/allennlp/interpret/attackers/hotflip.py"> [AllenNLP]</a></span>
			<span class="tag"><a href="https://github.com/thunlp/OpenAttack/blob/master/OpenAttack/attackers/hotflip.py"> [OpenAttack]</a></span>
		</td>
	</tbody>
	</table> -->
	
</div>





<div class="content anchor" >    
	<h3 class="content-section-list">Selected Awards and Grants</h3>  
		<p>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>
				<a style="color:#333" href="https://www.forbes.com/profile/rao-anyi/?list=30under30-asia-healthcare-science/">Forbes 30 Under 30 Asia</a></td> <td align="right"> 2025 </td></tr>
			<tr><td><li>
				Tencent Rhino-Bird Grant</td> <td align="right"> 2025 </td></tr>
			<tr><td><li>
				AIS Support Funding for Interdisciplinary Research Collaboration</td> <td align="right"> 2025 </td></tr>
				
			<tr><td><li>
				Bridge Gap Funding</td> <td align="right"> 2025 </td></tr>
			<tr><td><li>
				Rising Star by World Artificial Intelligence Conference (WAIC)  </td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				Talent Gift Funding </td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				Art Tech Funding </td> <td align="right"> 2024 </td></tr>
			
			<tr><td><li>
				Best Paper Award (Marr Prize) by International Conference on Computer Vision (ICCV)  </td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="https://brown.columbia.edu/announcing-2023-magic-grants/">
				Magic Grant</a> by The Brown Institute for Media Innovation </td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="https://www.primevideotech.com/">
				Amazon</a> Prime Video Gift Funding </td> <td align="right"> 2023 </td></tr>
			</li>
			</tbody>
			</table>

			<details><summary>More</summary>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>
				Tencnet, Cybever, and Pika Grant for SIGGRAPH Workshop Organization</td> <td align="right"> 2025 </td></tr>
			<tr><td><li>
				Adobe and Pika Grant for CVPR Workshop Organization</td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				Pika and KAUST Grant for ICCV Workshop Organization</td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				KAUST Grant for ECCV Workshop Organization</td> <td align="right"> 2022 </td></tr>
			<tr><td><li>
				Adobe Grant for ICCV Workshop Organization</td> <td align="right"> 2021 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="https://cerg1.ugc.edu.hk/hkpfs/index.html"> Hong Kong PhD Fellowship</a> </td> <td align="right"> 2021 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="https://www.paperdigest.org/2021/08/most-influential-acl-papers-2021-08/"> Most Influential Paper by Paper Digest</a> </td> <td align="right"> 2020 </td></tr>
			<tr><td><li><a style="color:#333" href="http://xgc.nju.edu.cn/08/f9/c1980a264441/page.htm"> Nanjing University Top-Grade Scholarship</a>,the highest honor in the university </td> <td align="right">2018</td></tr>
			<tr> <td><li>SenseTime Scholarship, awarded to 30 students out of all AI major undergraduate students in China</td> <td align="right"> 2017 </td></tr>
			<tr><td><li>Provincial Merit Student awarded by the Jiangsu Province </td> <td align="right"> 2017 </td></tr>
			<tr><td><li>National Scholarship awarded by the China Ministry of Education</td> <td align="right"> 2015 </td></tr>	
			<tr><td><li><a style="color:#333" href="./images/more/MathSeniorHigh.png">Gold Medal</a> in Invitational National Mathematical Olympiad</td> <td align="right"> 2013</td></tr>
			<tr><td><li>Nanjing University Outstanding Student Leader Award</td> <td align="right"> 2015</td></tr>
			<tr><td><li>Nanjing University Outstanding Student Award</td> <td align="right"> 2016</td></tr>
			<tr><td><li>Nanjing University Top Volunteer Excellence Award </td> <td align="right"> 2015</td></tr>
			<tr><td><li>Zhenggang Scholarship, top 40 students in Nanjing University </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Zhenggang Jingying Scholarship </td> <td align="right"> 2017</td></tr>
			<tr><td><li>Nanjing University People Scholarship</td> <td align="right"> 2016</td></tr>
			<tr><td><li>Nanjing University People Scholarship</td> <td align="right"> 2017</td></tr>
			<tr><td><li>World ranking 32nd in 2016 <a style="color:#333" href="https://www.youtube.com/watch?v=iO5runHvnoM"> Calculus World Cup</a> </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Meritorious winner prize in the 2016 National Mathematical Contest in Modeling  </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Best paper in the 2014 University Electronics Design Contest  </td> <td align="right">2014</td></tr>
			</li>
			</tbody>
			</table>
			</details>
	</p>
</div>    

	
<div class="content anchor" id="talks">
	<h3 class="content-section-list">Talks</h3>

	<p>
		<table style="border-spacing:2px" width="100%">
		<body>
		<tr><td><li>From Intention to Attention to Manifestation</td></tr>
			<tr><td>
				<ul><ul>
					<li>SIGGRAPH Asia, 12/2025</li>
					<li>World Cultural Forum, 11/2025</li>
					<li>SIGGRAPH, 08/2025</li>
				</ul></ul>
			</td></tr>

		<tr><td><li>Bridging the Representation Gap of Humans and Computers for Video Production</td></tr>
			<tr><td>
				<ul><ul>
					<li>China Golden Rooster Film Festival, 11/2024</li>
					<li>World Artificial Intelligence Conference (WAIC), 07/2024</li>
					<li>Shanghai Television Magnolia Festival, 06/2024</li>
					<li>University of Hong Kong (HKU), 05/2024</li>
					<li>Hong Kong University of Science and Technology (HKUST), 05/2024</li>
					<li>Stanford University, 04/2024</li>
					<li><a style="color:#333" href="https://datascience.hku.hk/hk-sh-ai-forum-2024/">Hong Kong Shanghai AI Forum</a>, 04/2024</li>
				</ul></ul>
			</td></tr>

		<tr><td><li>Collaborative Intelligent Tools to Support Video Production</td></tr>
			<tr><td>
				<ul><ul>
					<li><a style="color:#333" href="https://planet-ai-2023.github.io/2024/02/26/online-symposium-on-march-22-ai-technologies-and-their-implications/">Symposium on AI Technologies and Their Implications</a>, 03/2024</li>
					<li>Adobe, 03/2024</li>
					<li>CCF, 03/2024</li>
					<li>Film School of HKBU, 05/2021</li>
				</ul></ul>
			</td></tr>

		<tr><td><li>Creative Video Understanding, Editing and Generation</td></tr>
			<tr><td>
				<ul><ul>
					<li><a style="color:#333" href="https://planet-ai-2023.github.io/2023/12/31/webinar-feb-9-creative-video-understanding-editing-and-generation/">Art School of UTK</a>, 02/2024</li>
					<li>ICCV23 Workshop, Paris, France, 10/2022</li>
					
				</ul></ul>
			</td></tr>

		<tr><td><li>Controllable Visual Content Generation to Unleash Creativity and Productivity</td></tr>
			<tr><td>
				<ul><ul>
					<li>Netflix, 11/2023</li>
					<li>Bay Area Vision Day (Stanford, Berkeley, Caltech), 09/2023</li>
				</ul></ul>
			</td></tr>

		<tr><td><li>Temporal and Contextual Transformer for Multi-Camera Editing</td></tr>
			<tr><td>
				<ul><ul>
					<li>ECCV22 Workshop, 10/2022</li>
				</ul></ul>
			</td></tr>

		<tr><td><li>Multimodal Representation Learning</td></tr>
			<tr><td>
				<ul><ul>
					<li>Meta, Redmond, WA, 12/2022</li>
					<li>CVPR20 Workshop on Sight and Sound, 06/2020</li>
					<li>CVPR20 Workshop on Learning from Unlabeled Videos, 06/2020</li>
				</ul></ul>
			</td></tr>
				
		<tr><td><li>Cinematic Style Analysis Based on Subject Centric Lens</td></tr>
			<tr><td>
				<ul><ul>
					<li>ECCV20 Workshop on Video Turing Test: Human-level Video Story Understanding, 08/2020</li>
				</ul></ul>
			</td></tr>
		
		</tbody>
		</table>
	</p>

	<h3 class="content-section-list">Press Coverage</h3>
	<p>
		<li>
		<a style="color:#333" href="https://hkust.edu.hk/news/where-ai-meets-humanity-scholar-democratizing-storytelling">
			Where AI Meets Humanity: A Scholar Democratizing Storytelling: HKUST
		</a>
		</li>
		<li>
			AI Film Production: 
			<a style="color:#333" href="https://www.i-cable.com/%e6%96%b0%e8%81%9e%e8%b3%87%e8%a8%8a/383088/%E6%99%BA%E5%89%B5%E6%9C%AA%E4%BE%86-ai%E9%9B%BB%E5%BD%B1%E8%88%87%E5%BD%B1%E7%89%87%E7%94%9F%E6%88%90-2">
				i-CABLE News 
			</a> /
			<a style="color:#333" href="https://ais.hkust.edu.hk/whats-happening/news/i-cable-news-features-amc-faculty-and-iip-students-ai-film-innovation">
				HKUST
			</a>
			
		</li>
		<li>
			AI Film Festival: 
			<a style="color:#333" href="https://apnews.com/press-release/pr-newswire/movies-hong-kong-fairs-and-festivals-fda39770be0bab616dbb1e1710734a0a">Associated Press</a> /
			<a style="color:#333" href="https://finance.yahoo.com/news/ai-stage-hkust-debuts-ai-124000924.html?guccounter=1&guce_referrer=aHR0cHM6Ly9jdmV1LmdpdGh1Yi5pby8&guce_referrer_sig=AQAAADdLpQCNHn37u7g-a939Q5dQBWFx4Q6bOXW6oY0WbxsShJpwhVZqpp-VJlhRX5KxHRZEep7ZyfiW8lR5giSW-Nx4nWqhoi765m5mA3PQ0tycQKqQzE6rbnvemxf8e1ZZjSd8ZS5iGJZnZzbGl3ygmlpUInShfVTVvYHughH9RmOx">Yahoo</a> /
			<a style="color:#333" href="https://today.line.me/hk/v2/article/GgvNXwy">PR Newswire</a>
			<a style="color:#333" href="https://paper.hket.com/article/3937065/%E7%A7%91%E5%A4%A7%E3%80%8CAI%E9%9B%BB%E5%BD%B1%E7%AF%80%E3%80%8D 3%E5%A4%A7%E7%8D%8E%E5%91%A8%E5%85%AD%E6%8F%AD%E7%9B%85">HKET</a>
			<a style="color:#333" href="https://www.takungpao.com/news/232109/2025/0416/1078300.html">Ta Kung Pao</a>
			<a style="color:#333" href="https://www1.hkej.com/dailynews/finnews/article/4055775/%E5%85%A8AI%E8%A3%BD%E4%BD%9C%E9%9B%BB%E5%BD%B1%E7%AF%80+35%E9%83%A8%E7%89%87%E5%85%A5%E5%9C%8D">HK Economic Journal</a>
		</li>
		<li>
			Media/Entertainment Industry: 
			<a style="color:#333" href="https://mp.weixin.qq.com/s/jgoGuIuBg4j1fcH1llv9oA">China Golden Rooster Film Festival</a> /
			<a style="color:#333" href="http://www.stvf.com/content?aid=101240629180906576797936189444101611">Shanghai Television Magnolia Festival</a> /
			<a style="color:#333" href="https://www.sh.chinanews.com.cn/yule/2024-06-28/125966.shtml">China News</a>
		</li>
		<li>
			World AI Conference:
			<a style="color:#333" href="http://www.djcaijing.com/science/4000.html">Video Generation Forum</a> /
			<a style="color:#333" href="https://www.jiqizhixin.com/articles/2024-07-08-2">Rising Star</a>
			
		</li>
		<li>Art: 
			<a style="color:#333" href="https://knowyourmeme.com/memes/hidden-imagery-in-ai-art">Hidden Imagery In AI Art</a> /
			<a style="color:#333" href="https://arstechnica.com/information-technology/2023/09/dreamy-ai-generated-geometric-scenes-mesmerize-social-media-users/">Funky AI-generated Spiraling Medieval Village Captivates Social Media</a> 
		</li>
	</p>

</div>   


<div class="content anchor" id="service">
	<h3 class="content-section-list">Professional Activities</h3>
	<p>
		<li>Area Chair for CVPR26
		<li>Area Chair for ICLR26
		<li>Associate Chair for UIST26
		<li>Co-Chair (Workshop) for MMSys26 
		<li>Area Chair (TPC) for SIGGRAPH Asia26
		<li>Co-Chair (Registeration) for UIST25 
		<li>Area Chair (TPC) for SIGGRAPH Asia25
		<li>Co-Chair (Film) for CVM25
		<li>Co-Chair (Media Gallery) for VINCI25 
		<li>Co-Chair (Registeration) and Session Chair for UIST24 
		<li>Leading/key Curator: 
			2025 Hong Kong <a style="color:#333" href="https://cveu.github.io/event/hkustfilm2025.html">HKUST</a> AI Film Festival,
			2023 Paris <a style="color:#333" href="https://filmfreeway.com/AIShortFest">ShortFest</a> AI Film Festival. 
		<br>
		<li>Leading/key Organizer: 
		<a style="color:#333" href="https://cveu.github.io/">Workshop on AI for Creative Video Editing and Understanding</a> at SIGGRAPH25, CVPR25, CVPR24, ICCV23, ECCV22, ICCV21 
		<br>
		<li>Conference Reviewer: CVPR, ICCV, ECCV, ACCV, SIGGRAPH, SIGGRAPH Asia, Eurographics, CHI, UIST, MM, NeurIPS, ICML, ICLR, AAAI, IJCAI<br>
		<li>Journal Reviewer: IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 
			IEEE Transactions on Multimedia (TMM), 
			IEEE Transactions on Visualization and Computer Graphics (TVCG),
			IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), ACM Transactions on Graphics (TOG), 
			Transactions on Machine Learning Research (TMLR),
			International Journal of Computer Vision (IJCV)<br>
		<li>Judge: 
		<a style="color:#333" href="https://www.youtube.com/watch?v=IrBnULdzx1E&ab_channel=SenseTime%E5%95%86%E6%B9%AF%E7%A7%91%E6%8A%80">
		The 3rd International Artificial Intelligence Fair</a>
	</li>
	</p>
</div>

<div class="content anchor" id="experience">  	      
	<h3 class="content-section-list">Research Experiences</h3>
	<p>
		<li>
			Research Intern at <a style="color:#333" href="https://about.meta.com/realitylabs">Meta Reality Lab</a>
		</li>	
		<li>
			Research Intern at <a style="color:#333" href="https://www.shlab.org.cn">Shanghai Artificial Intelligence Laboratory</a>
		</li>	
		<li>
			Research Intern at <a style="color:#333" href="https://www.sensetime.com/en">SenseTime Research</a>
		</li>
		<li>
			Visitor at the 
			<a style="color:#333" href="https://web.cs.toronto.edu">University of Toronto</a> and 
			<a style="color:#333" href="https://vectorinstitute.ai">Vector Institute</a>
		</li>
		<li>
			Research Assistant at the 
			<a style="color:#333" href="http://aimlab.cs.uoregon.edu/publication.php">Advanced Integration and Mining Lab</a>, 
			Eugene, OR, USA
		</li>
		<li>
			Research Intern at 
			<a style="color:#333" href="https://www.cs.hku.hk/">University of Hong Kong</a>,
			Hong Kong S.A.R.
		</li>
	</p>
</div>
	
<div class="content" >
	<h3 class="content-section-list">Teaching Experiences</h3>
	<p>
		<table style="border-spacing:5px" width="100%">
		<body>
			HKUST AMCC 5140 AI for Visual Arts and Creativity (<span style="font-weight: bold;">AVAC</span>)<br>
			HKUST AMCC 5150 Visual Computing for Visual Arts and Creativity (<span style="font-weight: bold;">VCVAC</span>)<br>
			HKUST AMCC 5250 Filmmaking with AI Innovations (<span style="font-weight: bold;">FilmAI</span>)<br>
			HKUST AMCC 5000	Creative Convergence: Foundations of Arts and Machine Creativity<br>
			HKUST AMCC 6950A	Special Projects in Arts and Machine Creativity<br>
			HKUST EMIA 6950F	Independent Study<br>
			HKUST EMIA 6500K Visual Computing for Visual Content Creation (<span style="font-weight: bold;">VCVCC</span>) <br>
			HKUST EMIA 6500H AI for Visual Content Creation (<span style="font-weight: bold;">AIVCC</span>) <br>
			SIGGRAPH Asia 2025 <a href="https://cveu.github.io/">Course on Generative Models for Visual Content Editing and Creation</a><br>
			SIGGRAPH 2024 <a href="https://cveu.github.io/event/sig2024.html">Course on Generative Models for Visual Content Editing and Creation</a><br>
			CCF 2024 Advanced Disciplines Lectures
		</tbody>
		</table>
	</p>

	<h3 class="content-section-list">Patents</h3>
	<p>
		<table style="border-spacing:5px" width="100%">
		<tbody>
			A Video Generation Method, CN202210699177.X <br>
			A Video Editing Method and Related Program Products, CN202210691662.2 <br>
			A Video Editing Method, CN202010694551.1 <br>
			A Video Classification Method, CN202010694811.1 <br>
			An Image Processing Method and Related Products, CN202010450801.3  <br>
			A Zero-shot Action Recognition Method, CN202110821209.4 <br>
			A Layout Generation Method, CN202111128
		</tbody>
		</table>
	</p>

	
	
</div> 


<div class="content" >
	<h3 class="content-section-mis">Miscellaneous</h3>
		<p style="margin-bottom:-1mm;" class="content-subsection-mis"> <span style="font-weight: bold;">Undergrad</span><br>
			<details><summary>Click to expand</summary>

			<h4>Undergrad Academic</h4>
				<p style="margin-top:-1mm;">His GPA ranked No.1 in each semester during his undergraduate studies at Nanjing University
				with an overall GPA: 3.96/4.00 and Rank: 1/183.
				He finished major curricula in 2 years and learned a bunch of online courses.
					<a href="./all_undergrad_online"> [Whole]</a><br>
				</p>

			<h4>Undergrad Research Beginning</h4>
				<table style="margin-top:-3mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
					<tbody><tr valign="baseline">
						<td valign="middle" >
						<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Robust Training with Word-level Adversity for NLP</span> <br>
						<span style="font-style: italic;"> Sept. 2017 - April. 2018 </span> &nbsp  Advanced Integration and Mining Lab (AIM), Eugene, OR, United States of America<br>
						Advisor: Prof. <a href="http://ix.cs.uoregon.edu/~dou/">Dejing Dou</a>  (Director, Head of Baidu Big Data Lab) and Prof. <a href="http://ix.cs.uoregon.edu/~lowd/">Daniel Lowd</a>
						<span class="tag"><a href="https://www.theregister.co.uk/2018/06/28/machine_translation_vulnerable/"> [The Register]</a></span>
						</p>
						</td>
				</tbody>
				</table>

				<table style="margin-top:-2mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
				<tbody><tr valign="baseline">
					<td  valign="middle" >
					<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Automatic Music Accompaniment Using Probabilistic Machine Learning</span> <br>
					<span style="font-style: italic;"> Jul. 2017 - Aug. 2017 </span> &nbsp  The University of Hong Kong, Hong Kong S.A.R.<br>
					Advisor: Prof. <a href="http://www.cs.hku.hk/people/profile.jsp?teacher=fcmlau">Francis Lau</a> (Associate Dean)
					<span class="tag"><a href="https://arxiv.org/abs/1803.09033"> [Arxiv]</a></span> 
					</p>
					</td>
				</tbody>
				</table>

				<table style="margin-top:-3mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
				<tbody><tr valign="baseline">
					<td  valign="middle" >
					<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Real-time 3D Surface Reconstruction Using Lidar (Light Detection And Ranging)</span> <br>
					<span style="font-style: italic;"> Aug. 2016 - Sept. 2017 </span> &nbsp  Visual Sensing and Graphics Lab (VISG Lab), Nanjing University <br>
					Supervisor: Prof. <a href="https://ese.nju.edu.cn/dsd/list.htm">  Sidan Du</a> (Director)
					<span class="tag"><a href="papers/icra2018pre.pdf"> [Report]</a></span> 
					<span class="tag"><a href="https://youtu.be/_6JFl3p75qg"> [Video]</a></span>
					</li>	
					</p>
					</td>
				</tbody>
				</table>	

			<h4>Undergrad Course Projects</h4>
				Computer Vision:
					3D Human Poses Estimation from a Single Image
					<span class="tag"><a href="papers/CV_Report.pdf"> [Presentation]</a></span><br>
				Convex Optimization:
					Road Car Flow Prediction
					<span class="tag"><a href="papers/mcm.pdf"> [Report]</a></span><br>
				Probability and Stochastic Process:
					Monte Carlo for Multidimensional Integrals
					<span class="tag"><a href="papers/monte_carlo_method.pdf"> [Report]</a></span>	<br>
				Microcomputers and Interface Techniques: x86 Assembly Language Programming <span class="tag"><a href="papers/masm.pdf"> [Report]</a></span><br>
				Signal Processing: Single-Photon Detector Design
					<span class="tag"><a href="papers/basic_forum.pdf"> [Report]</a></span>
					<span class="tag"><a href="papers/Low_noise_preamplifier.pdf"> [Presentation]</a></span>
			</details>
		<br>

		<p style="margin-bottom:-1mm;" class="content-subsection-mis"> <span style="font-weight: bold;">Volunteer Experiences</span><br>
			<details><summary>Click to expand</summary>
			<p> <span style="font-weight: bold;"> Co-Founder </span> of a Children Care Volunteer Program  &nbsp &nbsp 
				<span style="font-style: italic;"> Sep. 2015 - Dec. 2015</span> <br>		
				Co-founded a psychological consulting program to promote left-behind children's growth and education.
				Volunteered to teach left-behind children Math and English in 
				a junior high school located in the remote, underdeveloped Xiushui county.
				Recognized as a key team leader in the successful Warm One Hundred Campaign, which raised money for
				left-behind children. Our group received an excellence award from the China Foundation for Poverty Alleviation.
			</p>

			<p> <span style="font-weight: bold;"> Vice President </span> of a Young Volunteers Association at Nanjing University &nbsp &nbsp
				<span style="font-style: italic;"> Jun. 2015 - Jun. 2016</span> <br>
				Organized and participated in over 100 out-of-school and 20 in-school activities covering over 1000 volunteers.
				Our association received a volunteer association excellence award.
			</p>

			<p> <span style="font-weight: bold;"> Campus Ambassador </span> of <a style="color:#333" href="http://www.huawei.com/en/">Huawei</a> &nbsp &nbsp
				<span style="font-style: italic;"> Aug. 2017 - Dec. 2017</span> <br>
			</p>

			<p> <span style="font-weight: bold;"> Student Volunteer </span> of International Conference on Computer Vision (ICCV) &nbsp &nbsp 
				<span style="font-style: italic;"> Dec. 2019</span> <br>
			</p>
			</details>
		<br>

	</div>

	<div style="text-align:center;">
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=2d78ad&w=250&t=tt&d=NoxTntTwijcGNPPsm7CM_ctT1E79_SAsBke-aS4Vw4Q&co=ffffff&ct=2d78ad&cmo=aa3939&cmn=ff5353'></script>
	</div>
</body>
